# -*- coding: utf-8 -*-
"""Model Training and Features Extraction Full_FYP_Code_Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19WK0uQEn17IbuMyrdolwDAkDt6D47Txr

# Basic Starter Steps

## Mount Google Drive
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""## Get the code from github"""

!git clone https://github.com/SyedaMahamFahim/Inner_Speech_Dataset -q

"""## Install dependencies

*   List item
*   List item



"""

!pip3 install mne -q
!pip install h5io

"""### Checking MNE version"""

import mne
print(mne.__version__)

"""## Imports

"""

import mne
import warnings
import numpy as np

from google.colab import drive

from Inner_Speech_Dataset.Python_Processing.Data_extractions import  extract_data_from_subject,extract_block_data_from_subject
from Inner_Speech_Dataset.Python_Processing.Data_processing import  select_time_window, transform_for_classificator,filter_by_condition,filter_by_class
from Inner_Speech_Dataset.Python_Processing.Utilitys import ensure_dir, unify_names

np.random.seed(23)

mne.set_log_level(verbose='warning') #to avoid info at terminal
warnings.filterwarnings(action = "ignore", category = DeprecationWarning )
warnings.filterwarnings(action = "ignore", category = FutureWarning )

"""## Variable for whole code"""

### Hyperparameters

# The root dir has to point to the folder that contains the database
root_dir = "/content/drive/MyDrive/FYPD_Dataset"
save_dir = "/content/drive/MyDrive/FinalFilterOuput"

# Data Type
datatype = "eeg" # 3

# Sampling rate
fs = 256

# Select the useful par of each trial. Time in seconds
t_start = 1.5
t_end = 3.5

"""## Ensuring Folder exist"""

import os

# Check if the directory exists, and if not, create it
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
    print(f"Created directory: {save_dir}")
else:
    print(f"Directory already exists: {save_dir}")

"""# **------------ Data extraction and filtration  ----------------**

## For Single File Block

### Variable
"""

# Subject number
N_B=2
N_S = 1   #[1 to 10]

"""### Data Extraction"""

# Load all trials for a single block
print("Extracted from block")
X, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
X = X.get_data()  # Convert EpochsFIF object to numpy array
print(X.shape)

# Cut useful time. i.e action interval
X = select_time_window(X = X, t_start = t_start, t_end = t_end, fs = fs)
print("Useful Time")
print(X.shape)

print("Data shape: [trials x channels x samples]")
print(X.shape) # Trials, channels, samples
print(X)
print("Labels shape")
print(Y.shape) # Time stamp, class , condition, session
# print(Y)

"""### Filter By condition i.e Inner"""

X_inner_speech, Y_inner_speech = filter_by_condition(X, Y, condition='Inner')

print(X_inner_speech.shape) # Trials, channels, samples
print(X_inner_speech[:2])
print("Labels shape")
print(Y_inner_speech.shape) # Time stamp, class , condition, session
print(Y_inner_speech[:5])
# here 3rd col is 1 because our condition is inner

"""### Filter By class i.e ALL classes"""

X_inner_speech, Y_inner_speech= filter_by_class(X_inner_speech, Y_inner_speech, 'ALL')
print(Y_inner_speech.shape)
print(Y_inner_speech[:5])

"""## For Single Block full code"""

# Load all trials for a single block
print("Extracted from block")
X, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
X = X.get_data()  # Convert EpochsFIF object to numpy array
print(X.shape)

# Cut useful time. i.e action interval
X = select_time_window(X = X, t_start = t_start, t_end = t_end, fs = fs)
print("Useful Time")
print(X.shape)

# Filter by Condition
X_inner_speech, Y_inner_speech = filter_by_condition(X, Y, condition='Inner')

print("Only Inner Condition")
print(X_inner_speech.shape) # Trials, channels, samples


# Filter by Class
X_inner_speech, Y_inner_speech= filter_by_class(X_inner_speech, Y_inner_speech, 'ALL')
print("ALL Classes")
print(Y_inner_speech.shape)

"""## For Single File

### Variable
"""

# Subject number
N_S = 1   #[1 to 10]

"""### Data Extraction"""

# Load all trials for a single subject
X, Y = extract_data_from_subject(root_dir, N_S, datatype)

# Cut useful time. i.e action interval
X = select_time_window(X = X, t_start = t_start, t_end = t_end, fs = fs)

print("Data shape: [trials x channels x samples]")
print(X.shape) # Trials, channels, samples
print(X[:2])
print("Labels shape")
print(Y.shape) # Time stamp, class , condition, session
print(Y)

"""### Filter By condition i.e Inner"""

X_inner_speech, Y_inner_speech = filter_by_condition(X, Y, condition='Inner')

print(X_inner_speech.shape) # Trials, channels, samples
print(X_inner_speech[:2])
print("Labels shape")
print(Y_inner_speech.shape) # Time stamp, class , condition, session
print(Y_inner_speech[:5])
# here 3rd col is 1 because our condition is inner

"""### Filter By class i.e ALL classes"""

X_inner_speech, Y_inner_speech= filter_by_class(X_inner_speech, Y_inner_speech, 'ALL')
print(Y_inner_speech.shape)
print(Y_inner_speech[:5])

"""## For Multpile Files

### Variables
"""

# The root dir has to point to the folder that contains the database
root_dir = "/content/drive/MyDrive/FYPD_Dataset"
save_dir = "/content/drive/MyDrive/FinalFilterOuput"

# Save options
save_bool = True
overwrite = True

# Subjets list for training
N_S_list = [1, 3, 4, 5, 6, 10]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3

"""### Applying Loop on all files to get Data"""

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")


    # Load all trials for a single subject
    X, Y = extract_data_from_subject(root_dir, N_S, datatype)

    # Cut useful time. i.e action interval
    X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

    # Filter By condition i.e Inner
    X, Y = filter_by_condition(X, Y, condition='Inner')

    # Filter By class i.e ALL classes
    X, Y = filter_by_class(X, Y, 'ALL')
    # Append the data to the lists
    all_features.append(X)
    all_labels.append(Y)

# Stack all the features and labels
X_data = np.vstack(all_features)
Y_data = np.vstack(all_labels)

print("Combined Features shape:", X_data.shape)
print("Combined Labels shape:", Y_data.shape)
print("First few combined features:\n", X_data[:5])
print("First few combined labels:\n", Y_data[:5])

"""# **--------------------------- Basic Features Extraction\------------------------------------**

## For Single File

### Variable
"""

# Subject number
N_S = 1   #[1 to 10]

"""### Data Extraction"""

# Load all trials for a single subject
X, Y = extract_data_from_subject(root_dir, N_S, datatype)

# Cut useful time. i.e action interval
X = select_time_window(X = X, t_start = t_start, t_end = t_end, fs = fs)

print("Data shape: [trials x channels x samples]")
print(X.shape) # Trials, channels, samples
print(X[:2])
print("Labels shape")
print(Y.shape) # Time stamp, class , condition, session
print(Y)

"""### Filter By condition i.e Inner"""

X, Y = filter_by_condition(X, Y, condition='Inner')

print(X.shape) # Trials, channels, samples
print(X[:2])
print("Labels shape")
print(Y.shape) # Time stamp, class , condition, session
print(Y[:5])
# here 3rd col is 1 because our condition is inner

"""### Filter By class i.e ALL classes"""

X, Y= filter_by_class(X, Y, 'ALL')
print(Y.shape)
print(Y[:5])

"""### Extract Features From Epochs"""

import numpy as np

# Assuming X is your data
# Example data shape: (num_epochs, num_channels, num_samples)
# Replace this with your actual data
# X = your_data

# Mean amplitude across all channels for each epoch
mean_amplitude = X.mean(axis=2)
print("Mean Amplitude:\n", mean_amplitude[:5])

# Standard deviation of amplitude across all channels for each epoch
std_amplitude = X.std(axis=2)
print("Standard Deviation Amplitude:\n", std_amplitude[:5])

# Variance of amplitude across all channels for each epoch
var_amplitude = X.var(axis=2)
print("Variance Amplitude:\n", var_amplitude[:5])

# Minimum amplitude across all channels for each epoch
min_amplitude = X.min(axis=2)
print("Minimum Amplitude:\n", min_amplitude[:5])

# Maximum amplitude across all channels for each epoch
max_amplitude = X.max(axis=2)
print("Maximum Amplitude:\n", max_amplitude[:5])

# Range (peak-to-peak) of amplitude across all channels for each epoch
ptp_amplitude = X.ptp(axis=2)
print("Peak-to-Peak Amplitude:\n", ptp_amplitude[:5])

# Index of minimum amplitude across all channels for each epoch
argmin_amplitude = X.argmin(axis=2)
print("Index of Minimum Amplitude:\n", argmin_amplitude[:5])

# Index of maximum amplitude across all channels for each epoch
argmax_amplitude = X.argmax(axis=2)
print("Index of Maximum Amplitude:\n", argmax_amplitude[:5])

# Combine extracted features into a feature matrix
features = np.hstack([mean_amplitude,
                      std_amplitude,
                      var_amplitude,
                      min_amplitude,
                      max_amplitude,
                      ptp_amplitude,
                      argmin_amplitude,
                      argmax_amplitude])

print("Features shape:", features.shape)
print("Features:\n", features[:5])

"""## For Multiple Files

### Variables
"""

# The root dir has to point to the folder that contains the database
root_dir = "/content/drive/MyDrive/FYPD_Dataset"
save_dir = "/content/drive/MyDrive/FinalFilterOuput"

# Save options
save_bool = True
overwrite = True

# Subjets list for training
N_S_list = [1, 3, 4, 5, 6, 10]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate

"""### Extract Feature Function"""

def extract_and_process_features(root_dir, N_S, t_start, t_end, fs):
    # Load all trials for a single subject
    X, Y = extract_data_from_subject(root_dir, N_S, datatype)

    # Cut useful time. i.e action interval
    X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

    # Filter By condition i.e Inner
    X, Y = filter_by_condition(X, Y, condition='Inner')

    # Filter By class i.e ALL classes
    X, Y = filter_by_class(X, Y, 'ALL')

    # Mean amplitude across all channels for each epoch
    mean_amplitude = X.mean(axis=2)

    # Standard deviation of amplitude across all channels for each epoch
    std_amplitude = X.std(axis=2)

    # Variance of amplitude across all channels for each epoch
    var_amplitude = X.var(axis=2)

    # Minimum amplitude across all channels for each epoch
    min_amplitude = X.min(axis=2)

    # Maximum amplitude across all channels for each epoch
    max_amplitude = X.max(axis=2)

    # Range (peak-to-peak) of amplitude across all channels for each epoch
    ptp_amplitude = X.ptp(axis=2)

    # Index of minimum amplitude across all channels for each epoch
    argmin_amplitude = X.argmin(axis=2)

    # Index of maximum amplitude across all channels for each epoch
    argmax_amplitude = X.argmax(axis=2)

    # Combine extracted features into a feature matrix
    features = np.hstack([mean_amplitude,
                          std_amplitude,
                          var_amplitude,
                          min_amplitude,
                          max_amplitude,
                          ptp_amplitude,
                          argmin_amplitude,
                          argmax_amplitude])

    return features, Y

"""### Applying Loop on all files to get Data"""

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")

    # Extract and process features for each subject
    features, labels = extract_and_process_features(root_dir, N_S, t_start, t_end, fs)

    # Append the data to the lists
    all_features.append(features)
    all_labels.append(labels)

# Stack all the features and labels
X_data = np.vstack(all_features)
Y_data = np.vstack(all_labels)

print("Combined Features shape:", X_data.shape)
print("Combined Labels shape:", Y_data.shape)
print("First few combined features:\n", X_data[:5])
print("First few combined labels:\n", Y_data[:5])

"""### Saving File"""

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput"
    file_name = save_dir + "/Basic_Features_Combined.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

"""### Full Code"""

import numpy as np

### ---- Processing Variables ---- ###

# The root dir has to point to the folder that contains the database
root_dir = "/content/drive/MyDrive/FYPD_Dataset"
save_dir = "/content/drive/MyDrive/FinalFilterOuput"

# Save options
save_bool = True
overwrite = True

# Subjets list for training
N_S_list = [1, 3, 4, 5, 6, 10]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate

### Define Functions ###

def extract_and_process_features(root_dir, N_S, t_start, t_end, fs):
    # Load all trials for a single subject
    X, Y = extract_data_from_subject(root_dir, N_S, datatype)

    # Cut useful time. i.e action interval
    X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

    # Filter By condition i.e Inner
    X, Y = filter_by_condition(X, Y, condition='Inner')

    # Filter By class i.e ALL classes
    X, Y = filter_by_class(X, Y, 'ALL')

    # Mean amplitude across all channels for each epoch
    mean_amplitude = X.mean(axis=2)

    # Standard deviation of amplitude across all channels for each epoch
    std_amplitude = X.std(axis=2)

    # Variance of amplitude across all channels for each epoch
    var_amplitude = X.var(axis=2)

    # Minimum amplitude across all channels for each epoch
    min_amplitude = X.min(axis=2)

    # Maximum amplitude across all channels for each epoch
    max_amplitude = X.max(axis=2)

    # Range (peak-to-peak) of amplitude across all channels for each epoch
    ptp_amplitude = X.ptp(axis=2)

    # Index of minimum amplitude across all channels for each epoch
    argmin_amplitude = X.argmin(axis=2)

    # Index of maximum amplitude across all channels for each epoch
    argmax_amplitude = X.argmax(axis=2)

    # Combine extracted features into a feature matrix
    features = np.hstack([mean_amplitude,
                          std_amplitude,
                          var_amplitude,
                          min_amplitude,
                          max_amplitude,
                          ptp_amplitude,
                          argmin_amplitude,
                          argmax_amplitude])

    return features, Y

### ---- Main Processing Loop ---- ###

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")

    # Extract and process features for each subject
    features, labels = extract_and_process_features(root_dir, N_S, t_start, t_end, fs)

    # Append the data to the lists
    all_features.append(features)
    all_labels.append(labels)

# Stack all the features and labels
X_data = np.vstack(all_features)
Y_data = np.vstack(all_labels)

print("Combined Features shape:", X_data.shape)
print("Combined Labels shape:", Y_data.shape)
print("First few combined features:\n", X_data[:5])
print("First few combined labels:\n", Y_data[:5])

### ---- Save the Combined Data ---- ###

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput"
    file_name = save_dir + "/Combined_Features.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

"""# **--------------------------- FOR PSD ------------------------------------**

## Computer PSD On A Single File

### Processing Variables
"""

# Data filtering
datatype = "eeg"

N_S=1
N_B=1

# Time window
tmin = 0.5
tmax = 3

# PSD Parameters
fmin = 0.5
fmax = 100
n_overlap = 0
n_fft = 256
picks = "all"
average = "mean"

"""### Data Extraction and Filtering"""

X_S, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
print("X_S.get_montage()")
print(X_S.get_montage())
print(X_S.metadata)
print(X_S.ch_names)
(X_data.shape[1])
channel_name=X_S.ch_names
print(channel_name)

# Set Montage
Adquisition_eq = "biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)
# print(X_S._data )

X, Y = extract_data_from_subject(root_dir, N_S, datatype)

X, Y= filter_by_condition(X, Y, condition='Inner')


print("Features")
print(X[:1])
print("Labels")
print(Y[:1])
# X_data = X
# # X_S._data = X_data

"""### Compute PSD"""

# print(X_S)
# print(X_S)

# Create an info object
info = mne.create_info(ch_names=[str(i) for i in range(X.shape[1])],
                       sfreq=256, ch_types="eeg")
print("Info",info)

# Create a new Epochs object with the combined data
epochs_combined = mne.EpochsArray(X, info)
print('epochs_combined',epochs_combined)
# Set the montage
montage = mne.channels.make_standard_montage("biosemi128")
print(montage)
epochs_combined.set_montage(montage)
# Calculate PSD for a particular class in a
# particular condition for the selected subjects
# print("Calculated PSD for Class: " + Classes + " in Condition: " + Cond)
# print("with the information of Subjects: " + str(N_S_list))
# Try to compute PSD and see what it returns directly
# psd_result = X.compute_psd(method='welch', fmin=fmin, fmax=fmax, tmin=tmin, tmax=tmax, picks=picks)

"""### Print PSD Result"""

# Now print what psd_result contains
print(type(psd_result))
print(psd_result.average())

print(Y[:10])

# Extract PSD values and frequency bins from the Power Spectrum object
psds = psd_result.get_data()  # This method typically exists for MNE data container objects; adjust if needed
freqs = psd_result.freqs  # This should be an attribute or similar; check documentation for exact usage

print("psds")

print(psds[:1])
print("freqs")
print(freqs[:2])
# Printing shapes and some data to confirm
print("PSDs shape:", psds.shape)
# print("First few PSD values for the first channel:", psds[0, 0, :])
# print("Frequency bins:", freqs)

# Print basic statistics of the PSD data
print("PSD Data Statistics:")
print("Mean:", psds.mean())
print("Standard Deviation:", psds.std())
print("Min:", psds.min())
print("Max:", psds.max())

"""### Save PSD File"""

# Save PSD results
file_name = save_dir + "/"+ "PSD_" + str(N_S) + "_PSD-tfr.npz"

print(file_name)
# # Save the data to a single npz file
np.savez(file_name, psds=psds, freqs=freqs,labels=Y)

"""### Plot PSD"""

import matplotlib.pyplot as plt

# # Choose a channel to plot
channel_index = 0  # Change as needed
plt.figure(figsize=(10, 5))
plt.plot(freqs, psds[0, channel_index, :], label=f'Channel {channel_index+1}')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (dB)')
plt.title('Power Spectral Density across Frequencies')
plt.legend()
plt.show()

"""### Checking Saved PSD file .h5 format

"""

import h5py

# Define the file path for the single PSD file you have processed
file_path = '/content/drive/MyDrive/FinalFilterOuput/PSD_Subject_1_Inner_ALL_PSD-tfr.h5'

# Load the data from the HDF5 file
with h5py.File(file_path, 'r') as f:
    # Load the PSD data
    psds = f['mnepython/key_data'][:]
    # You may need to adjust these paths based on your actual structure
    freqs = f['mnepython/key_dims/idx_2'][:]  # Assuming these are frequency bins
    # Check if there are labels stored in the file, you may need to adjust the path
    labels = f['mnepython/key_dims/idx_0'][:] if 'mnepython/key_dims/idx_0' in f else None

# Print the shape of the loaded data
print("Loaded PSDs shape:", psds.shape)
print("Loaded Frequency bins:", freqs.shape)

if labels is not None:
    print("Loaded Labels shape:", labels.shape)
    print("First few labels:", labels[:5])

# Print basic statistics of the PSD data
print("PSD Data Statistics:")
print("Mean:", psds.mean())
print("Standard Deviation:", psds.std())
print("Min:", psds.min())
print("Max:", psds.max())

# Example of printing the first few PSD values for the first channel
print("First few PSD values for the first channel:", psds[0, 0, :10])

"""## Computer PSD On A Multiple Files

#### Processing Variables
"""

import numpy as np
import mne

### ---- Processing Variables ---- ###

# Save options
save_bool = False
overwrite = True

# Subjets list
N_S_list = [1,2, 3, 4, 5, 6,7,8,9,10]  # for training
# N_S_list = [2,7,8,9]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
tmin = 0.5
tmax = 3

# PSD Parameters
fmin = 0.5
fmax = 100
n_overlap = 0
n_fft = 256
picks = "all"
average = "mean"

# Load data from the first subject to get the channel names
N_S = 1
N_B = 1

"""### Set up the EEG data structure and Set Montage


"""

# Load a single subject's data to set up the EEG data structure.
# This initial loading is crucial for establishing a framework for how EEG data should be handled in subsequent analyses.
# It involves:
# 1. Loading EEG data from one block of one subject to instantiate an Epochs object.
#    This object is used to store and manipulate segments of the EEG data consistently.
# 2. Setting the montage with 'biosemi128', a standard layout for EEG electrodes.
#    The montage ensures the software knows the exact location of each electrode on the scalp,
#    which is essential for accurate spatial analysis of EEG signals.
# This process sets up a template for processing additional EEG data with uniform settings and methods.


# Load data from the first subject to get the channel names
N_S = 1
N_B = 1

X_S, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
print("Extracting Data from 1 block to get channel names")
channel_names = X_S.ch_names
print("Channel names:", channel_names)

# Initialize an empty list to hold all the epochs data
all_epochs_data = []
all_labels = []

"""### Applying Loop on all files to get Data"""

# Loop through all subjects
for N_S in N_S_list:
    print("Subject: " + str(N_S))

    # Load the data for the subject
    X_s, Y = extract_data_from_subject(root_dir, N_S, datatype="eeg")

    # Filter by condition
    X_cond, Y_cond = filter_by_condition(X_s, Y, condition=Cond)

    # Filter by class
    X_cond, Y_cond = filter_by_class(X_cond, Y_cond, Classes)

    # Append the data to the list
    all_epochs_data.append(X_cond)
    all_labels.append(Y_cond)

    print("X_cond shape:", X_cond.shape)
    print("Y_cond shape:", Y_cond.shape)
    print("First few X_cond values:", X_cond[:2])
    print("First few Y_cond values:", Y_cond[:2])

# Stack all the data
X_data = np.vstack(all_epochs_data)
Y_data = np.vstack(all_labels)
print("Combined X_data shape:", X_data.shape)
print("Combined Y_data shape:", Y_data.shape)
print("First few X_data values:", X_data[:2])
print("First few Y_data values:", Y_data[:2])

"""### Creating MNE Info object"""

# Create an MNE Info object with correct channel names
info = mne.create_info(ch_names=channel_names, sfreq=256, ch_types="eeg")
print("MNE Info object created with channel names and sampling frequency.")
print(info)

# Create the epochs object
epochs_combined = mne.EpochsArray(X_data, info)
print("EpochsArray object created with combined data.")
print(epochs_combined)
# Set the montage
montage = mne.channels.make_standard_montage("biosemi128")
print("Montage object created with biosemi128 montage.")
print(montage)
epochs_combined.set_montage(montage, match_case=False)
print("Montage set for the EpochsArray object.")
print(epochs_combined)

"""### Calculate PSD"""

# Calculate PSD
print("Calculating PSD for the combined data...")
psd_result = epochs_combined.compute_psd(method='welch', fmin=fmin, fmax=fmax, tmin=tmin, tmax=tmax, picks=picks)

"""### Display PSD Data"""

# Extract PSD values and frequency bins from the Power Spectrum object
psds = psd_result.get_data()
freqs = psd_result.freqs

# Print some statistics for verification
print("PSDs shape:", psds.shape)
print("Frequency bins shape:", freqs.shape)
print("First few PSD values for the first channel:", psds[0, 0, :10])

"""### Save PSD"""

# Save PSD results
save_dir = "/content/drive/MyDrive/FinalFilterOuput"
file_name = save_dir + "/Testing_PSD_combined_PSD-tfr.npz"
print("Saving PSD results to:", file_name)
np.savez(file_name, psds=psds, freqs=freqs, labels=Y_data)

print("PSD calculation and saving completed.")

"""### Combine File

"""

import numpy as np

# Define file paths
training_file_path = '/content/drive/MyDrive/FinalFilterOuput/PSD_combined_PSD-tfr.npz'
testing_file_path = '/content/drive/MyDrive/FinalFilterOuput/Testing_PSD_combined_PSD-tfr.npz'
combined_file_path = '/content/drive/MyDrive/FinalFilterOuput/All_Subjects_Combined_PSD-tfr.npz'

# Load the training data
training_data = np.load(training_file_path)
psds_train = training_data['psds']
freqs_train = training_data['freqs']
labels_train = training_data['labels']

# Load the testing data
testing_data = np.load(testing_file_path)
psds_test = testing_data['psds']
freqs_test = testing_data['freqs']
labels_test = testing_data['labels']

# Check if frequency bins are the same
if not np.array_equal(freqs_train, freqs_test):
    raise ValueError("Frequency bins in training and testing data do not match.")

# Combine the PSDs and labels
psds_combined = np.concatenate((psds_train, psds_test), axis=0)
labels_combined = np.concatenate((labels_train, labels_test), axis=0)

# Save the combined data into a new npz file
np.savez(combined_file_path, psds=psds_combined, freqs=freqs_train, labels=labels_combined)

print("Combined PSDs shape:", psds_combined.shape)
print("Combined labels shape:", labels_combined.shape)
print("Combined data saved to:", combined_file_path)

"""### Full Code"""

import numpy as np
import mne

### ---- Processing Variables ---- ###

# Save options
save_bool = False
overwrite = True

# Subjets list
N_S_list=[2]
# N_S_list = [1,2, 3, 4, 5, 6,7,8,9,10]  # for training
# N_S_list = [2,7,8,9]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
tmin = 0.5
tmax = 3

# PSD Parameters
fmin = 0.5
fmax = 100
n_overlap = 0
n_fft = 256
picks = "all"
average = "mean"

# Load data from the first subject to get the channel names
N_S = 1
N_B = 1

X_S, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
print("Extracting Data from 1 block to get channel names")
channel_names = X_S.ch_names
print("Channel names:", channel_names)

# Initialize an empty list to hold all the epochs data
all_epochs_data = []
all_labels = []

# Loop through all subjects
for N_S in N_S_list:
    print("Subject: " + str(N_S))

    # Load the data for the subject
    X_s, Y = extract_data_from_subject(root_dir, N_S, datatype="eeg")

    # Filter by condition
    X_cond, Y_cond = filter_by_condition(X_s, Y, condition=Cond)

    # Filter by class
    X_cond, Y_cond = filter_by_class(X_cond, Y_cond, Classes)

    # Append the data to the list
    all_epochs_data.append(X_cond)
    all_labels.append(Y_cond)

    print("X_cond shape:", X_cond.shape)
    print("Y_cond shape:", Y_cond.shape)
    print("First few X_cond values:", X_cond[:2])
    print("First few Y_cond values:", Y_cond[:2])

# Stack all the data
X_data = np.vstack(all_epochs_data)
Y_data = np.vstack(all_labels)
print("Combined X_data shape:", X_data.shape)
print("Combined Y_data shape:", Y_data.shape)
print("First few X_data values:", X_data[:2])
print("First few Y_data values:", Y_data[:2])

# Create an MNE Info object with correct channel names
info = mne.create_info(ch_names=channel_names, sfreq=256, ch_types="eeg")
print("MNE Info object created with channel names and sampling frequency.")
print(info)

# Create the epochs object
epochs_combined = mne.EpochsArray(X_data, info)
print("EpochsArray object created with combined data.")
print(epochs_combined)
# Set the montage
montage = mne.channels.make_standard_montage("biosemi128")
print("Montage object created with biosemi128 montage.")
print(montage)
epochs_combined.set_montage(montage, match_case=False)
print("Montage set for the EpochsArray object.")
print(epochs_combined)
# Calculate PSD
print("Calculating PSD for the combined data...")
psd_result = epochs_combined.compute_psd(method='welch', fmin=fmin, fmax=fmax, tmin=tmin, tmax=tmax, picks=picks)

# Extract PSD values and frequency bins from the Power Spectrum object
psds = psd_result.get_data()
freqs = psd_result.freqs

# Print some statistics for verification
print("PSDs shape:", psds.shape)
print("Frequency bins shape:", freqs.shape)
print("First few PSD values for the first channel:", psds[0, 0, :10])

# Save PSD results
save_dir = "/content/drive/MyDrive/FinalFilterOuput"
file_name = save_dir + "/All_Subjects_PSD_combined_PSD-tfr.npz"
print("Saving PSD results to:", file_name)
np.savez(file_name, psds=psds, freqs=freqs, labels=Y_data)

print("PSD calculation and saving completed.")

"""## Model Train

### Model Train on Single file

#### Load the Data
"""

import numpy as np

# Define the path to load the npz file
npz_load_path = '/content/drive/MyDrive/FinalFilterOuput/PSD_combined_PSD-tfr.npz'

# Load the data from the npz file
data = np.load(npz_load_path)

print(data)
# # Extract PSDs and frequency bins
psds = data['psds']
freqs = data['freqs']
labels= data['labels']

# Print the shape of the loaded data

print("Loaded PSDs shape:", psds.shape)
print(psds[:1])
print("Loaded Frequency bins shape:", freqs.shape)
print(freqs[:10])
print("Loaded Labels shape:", labels.shape)
print(labels[:10])

"""#### Spliting Dataset

"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam


# Reshape PSD data for CNN input
# Current shape is (200, 128, 249)
# Required shape is (200, 128, 249, 1) for CNN input
psds = psds[..., np.newaxis]

print(psds.shape)
# One-hot encode labels (assuming labels are in the second column of the labels array)
encoder = OneHotEncoder(sparse=False)
labels_one_hot = encoder.fit_transform(labels[:, 1].reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(psds, labels_one_hot, test_size=0.2, random_state=42)

print("Training data shape:", X_train.shape)
print("Testing data shape:", X_test.shape)
print("Training labels shape:", y_train.shape)
print("Testing labels shape:", y_test[:10])
print("Testing labels shape:", y_test.shape)

"""#### Train Model"""

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 249, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(np.unique(labels[:, 1])), activation='softmax')  # Number of classes should match the number of unique labels
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

"""## Model training on testing and training file"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Define file paths
training_file_path = '/content/drive/MyDrive/FinalFilterOuput/PSD_combined_PSD-tfr.npz'
testing_file_path = '/content/drive/MyDrive/FinalFilterOuput/Testing_PSD_combined_PSD-tfr.npz'

# Load the training data
training_data = np.load(training_file_path)
psds_train = training_data['psds']
freqs_train = training_data['freqs']
labels_train = training_data['labels']

# Load the testing data
testing_data = np.load(testing_file_path)
psds_test = testing_data['psds']
freqs_test = testing_data['freqs']
labels_test = testing_data['labels']

# Print data shapes
print("Training PSDs shape:", psds_train.shape)
print("Training labels shape:", labels_train.shape)
print("Testing PSDs shape:", psds_test.shape)
print("Testing labels shape:", labels_test.shape)

# Reshape PSD data for CNN input
psds_train = psds_train[..., np.newaxis]
psds_test = psds_test[..., np.newaxis]

# One-hot encode labels (assuming labels are in the second column of the labels array)
encoder = OneHotEncoder(sparse=False)
labels_train_one_hot = encoder.fit_transform(labels_train[:, 1].reshape(-1, 1))
labels_test_one_hot = encoder.transform(labels_test[:, 1].reshape(-1, 1))

# Print reshaped data shapes
print("Reshaped training PSDs shape:", psds_train.shape)
print("Reshaped testing PSDs shape:", psds_test.shape)
print("One-hot encoded training labels shape:", labels_train_one_hot.shape)
print("One-hot encoded testing labels shape:", labels_test_one_hot.shape)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(psds_train, labels_train_one_hot, test_size=0.2, random_state=42)

# Print shapes
print("Training data shape:", X_train.shape)
print("Validation data shape:", X_val.shape)
print("Training labels shape:", y_train.shape)
print("Validation labels shape:", y_val.shape)

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 249, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(labels_train_one_hot.shape[1], activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))



# Print model summary
model.summary()


loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(psds_test, labels_test_one_hot)
print(f"Input Accuracy: {test_accuracy * 100:.2f}%")

"""# **--------------------------- FOR CSP \------------------------------------**

### Extract CSP Function
"""

def extract_features_using_csp(root_dir, N_S, t_start, t_end, fs):
    try:
        # Load all trials for a single subject
        X, Y = extract_data_from_subject(root_dir, N_S, datatype)

        # Cut useful time. i.e action interval
        X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

        # Filter By condition i.e Inner
        X, Y = filter_by_condition(X, Y, condition='Inner')

        # Filter By class i.e ALL classes
        X, Y = filter_by_class(X, Y, 'ALL')

        # Define CSP parameters
        n_components = 50  # Number of CSP components
        reg_param = 0.01

        # Initialize CSP with regularization
        csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)

        # Fit CSP on training data and transform data
        X_csp = csp.fit_transform(X, Y[:, 1])  # Assuming class labels are in the second column of Y

        return X_csp, Y
    except Exception as e:
        print(f"Error processing subject {N_S}: {e}")
        return None, None

"""## Single File

phly data extract hoga then CSP lagy ga
"""

from mne.decoding import CSP

def extract_features_using_csp_single(file_path):
    try:
        # Load epoch data
        epochs = mne.read_epochs(file_path)

        # Define the range of parameters to try
        n_components_values = [10, 20, 30,40,50]  # Adjust as needed
        reg_param_values = [0.01, 0.1, 0.5]  # Adjust as needed

        for n_components in n_components_values:
            for reg_param in reg_param_values:
                print("Trying n_components =", n_components, "and reg_param =", reg_param)

                # Initialize CSP with regularization
                csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)

                X = epochs.get_data()
                y = epochs.events[:, -1]  # Event labels

                try:
                    # Fit CSP on training data and transform data
                    X_csp = csp.fit_transform(X, y)
                    print("SVD computation successful for n_components =", n_components, "and reg_param =", reg_param)
                    return X_csp, y
                except Exception as e:
                    print("Error during SVD computation:", e)

        print("Unable to find suitable n_components and reg_param values.")
        return None, None

    except Exception as e:
        print("Error:", e)
        return None, None

file_name = '/content/drive/MyDrive/FYPD_Dataset/derivatives/sub-01/ses-01/sub-01_ses-01_baseline-epo.fif'
extract_features_using_csp_single(file_name)

"""## For Multiple Files

### Variables
"""

### ---- Processing Variables ---- ###

# Save options
save_bool = True
overwrite = True

# Subjects list
N_S_list = [1, 2,3, 4, 5, 6,7,8,9, 10]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate

"""### Loop

"""

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")

    # Extract and process features for each subject
    features, labels = extract_features_using_csp(root_dir, N_S, t_start, t_end, fs)

    if features is not None and labels is not None:
        # Append the data to the lists
        all_features.append(features)
        all_labels.append(labels)

# Stack all the features and labels
X_data = np.vstack(all_features)
Y_data = np.vstack(all_labels)

print("Combined Features shape:", X_data.shape)
print("Combined Labels shape:", Y_data.shape)
print("First few combined features:\n", X_data[:5])
print("First few combined labels:\n", Y_data[:5])

"""### Save the Combined Data"""

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput"
    file_name = save_dir + "/Combined_CSP_Features.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

"""## Full Code with 1 No. of component for training"""

import numpy as np
import mne
from mne.decoding import CSP

### ---- Processing Variables ---- ###

# Save options
save_bool = True
overwrite = True

# Subjects list for training
N_S_list = [1,2,3,7]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate


# Define CSP parameters
n_components = 50  # Number of CSP components
reg_param = 0.01


### Define Functions ###

def extract_features_using_csp(root_dir, N_S, t_start, t_end, fs):
    try:
        # Load all trials for a single subject
        X, Y = extract_data_from_subject(root_dir, N_S, datatype)

        # Cut useful time. i.e action interval
        X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

        # Filter By condition i.e Inner
        X, Y = filter_by_condition(X, Y, condition='Inner')

        # Filter By class i.e ALL classes
        X, Y = filter_by_class(X, Y, 'ALL')

        # Initialize CSP with regularization
        csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)

        # Fit CSP on training data and transform data
        X_csp = csp.fit_transform(X, Y[:, 1])  # Assuming class labels are in the second column of Y

        return X_csp, Y
    except Exception as e:
        print(f"Error processing subject {N_S}: {e}")
        return None, None

### ---- Main Processing Loop ---- ###

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")

    # Extract and process features for each subject
    features, labels = extract_features_using_csp(root_dir, N_S, t_start, t_end, fs)

    if features is not None and labels is not None:
        # Append the data to the lists
        all_features.append(features)
        all_labels.append(labels)

# Stack all the features and labels
X_data = np.vstack(all_features)
Y_data = np.vstack(all_labels)

print("Combined Features shape:", X_data.shape)
print("Combined Labels shape:", Y_data.shape)
print("First few combined features:\n", X_data[:1])
print("First few combined labels:\n", Y_data[:1])


### ---- Save the Combined Data ---- ###

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput/subjects"
    file_name = save_dir + "/CombinedFile_6Subjects_50_testing_CSP_Features.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

### ---- Save the Combined Data ---- ###

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput"
    file_name = save_dir + "/CombinedFile_6Subjects_50_testing_CSP_Features.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

"""## Full Code CSP Extraction for Subjects"""

import numpy as np
import mne
from mne.decoding import CSP

### ---- Processing Variables ---- ###

# Save options
save_bool = True
overwrite = True

# Subjects list for training
N_S_list = [1,2,3,4,5,6,7,8,9,10]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate

# Define CSP parameters
n_components = 50  # Number of CSP components
reg_param = 0.01

### Define Functions ###

def extract_features_using_csp(root_dir, N_S, t_start, t_end, fs):
    try:
        # Load all trials for a single subject
        X, Y = extract_data_from_subject(root_dir, N_S, datatype)

        # Cut useful time. i.e action interval
        X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

        # Filter By condition i.e Inner
        X, Y = filter_by_condition(X, Y, condition='Inner')

        # Filter By class i.e ALL classes
        X, Y = filter_by_class(X, Y, 'ALL')

        # Initialize CSP with regularization
        csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)

        # Fit CSP on training data and transform data
        X_csp = csp.fit_transform(X, Y[:, 1])  # Assuming class labels are in the second column of Y

        return X_csp, Y
    except Exception as e:
        print(f"Error processing subject {N_S}: {e}")
        return None, None

### ---- Main Processing Loop ---- ###

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")

    # Extract and process features for each subject
    features, labels = extract_features_using_csp(root_dir, N_S, t_start, t_end, fs)

    if features is not None and labels is not None:
        # Save the data for the individual subject
        save_dir = "/content/drive/MyDrive/FinalFilterOuput/subjects/all_csp_50_subjects"
        file_name = save_dir + f"/Subject_{N_S}_50_CSP_Features.npz"
        print(f"Saving features and labels for subject {N_S} to: {file_name}")
        np.savez(file_name, features=features, labels=labels)
        print("Saving completed for subject:", N_S)

### ---- Save the Combined Data ---- ###

if save_bool:
    save_dir = "/content/drive/MyDrive/FinalFilterOuput"
    file_name = save_dir + "/CombinedFile_6Subjects_50_testing_CSP_Features.npz"
    print(f"Saving combined features and labels to: {file_name}")
    np.savez(file_name, features=X_data, labels=Y_data)
    print("Saving completed.")

"""## Full Code with blocks"""

import numpy as np
import mne
from mne.decoding import CSP

### ---- Processing Variables ---- ###

# Save options
save_bool = True
overwrite = True

# Subjects list for training
N_S_list = [1,2,3,7]

# Data filtering
datatype = "eeg"
Cond = "Inner"
Classes = "ALL"

# Time window
t_start = 0.5
t_end = 3
fs = 256  # Sampling rate


# Define CSP parameters
n_components = 50  # Number of CSP components
reg_param = 0.01


### Define Functions ###

data = dict()
y = dict()
n_b_arr = [1, 2, 3]
datatype = datatype.lower()

def extract_features_using_csp(root_dir, N_S, N_B):
    try:
        # .fif
        # Load all trials for a single subject
        X, Y = extract_block_data_from_subject(root_dir, N_S, datatype, N_B)
        print("Extracted from block")

        X = X.get_data()  # Convert EpochsFIF object to numpy array
        print("X shape:", X.shape)
        print("Y shape:", Y.shape)


        # Cut useful time. i.e action interval
        print("startijg cut")
        X = select_time_window(X, t_start, t_end, fs)
        print("Cut useful time")
        print("X shape:", X.shape)
        print("Y shape:", Y.shape)


        # # Filter By condition i.e Inner
        X, Y = filter_by_condition(X, Y, condition='Inner')
        print("Filtered by condition")
        print("X shape:", X.shape)
        print("Y shape:", Y.shape)

        # # Filter By class i.e ALL classes
        X, Y = filter_by_class(X, Y, 'ALL')
        print("Filtered by class")
        print("X shape:", X.shape)
        print("Y shape:", Y.shape)


        # Initialize CSP with regularization
        csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)
        print("Initialized CSP")

        # Fit CSP on training data and transform data
        X_csp = csp.fit_transform(X, Y[:, 1])  # Assuming class labels are in the second column of Y

        return X_csp, Y
    except Exception as e:
        print(f"Error processing subject {N_S}: {e}")
        return None, None



### ---- Main Processing Loop ---- ###

# Initialize lists to hold all features and labels
all_features = []
all_labels = []

for N_S in N_S_list:
    print(f"Processing Subject: {N_S}")
    for N_B in n_b_arr:
        # Extract and process features for each subject
        features, labels = extract_features_using_csp(root_dir, N_S, N_B)
        if features is not None and labels is not None:
            # Append the data to the lists
            all_features.append(features)
            all_labels.append(labels)

        # Stack all the features and labels
        X_data = np.vstack(all_features)
        Y_data = np.vstack(all_labels)
        print("Combined Features shape:", X_data.shape)
        print("Combined Labels shape:", Y_data.shape)
        print("First few combined features:\n", X_data[:1])
        print("First few combined labels:\n", Y_data[:1])

        ### ---- Save the Combined Data ---- ###

        if save_bool:
            save_dir = "/content/drive/MyDrive/FinalFilterOuput/blocks/testing_blocks"
            # subject_dir = os.path.join(save_dir, f"subject_{N_S}_block_{N_B}")
            file_name = os.path.join(save_dir, f"subject_{N_S}_block_{N_B}_testing_50_CSP_Features.npz")
            print(file_name)
            print(f"Saving combined features and labels to: {file_name}")
            np.savez(file_name, features=X_data, labels=Y_data)
            print("Saving completed.")



"""## Full Code with multiple no. of components"""

from mne.decoding import CSP
import numpy as np

# Define the processing variables
root_dir = "/content/drive/MyDrive/FYPD_Dataset"
save_dir = "/content/drive/MyDrive/FinalFilterOuput"
N_S_list = [1, 3, 4, 5, 6, 10]
t_start = 0.5
t_end = 3
fs = 256
datatype = "eeg"

def extract_features_using_csp(root_dir, N_S_list, t_start, t_end, fs):
    try:
        # Define the range of parameters to try
        n_components_values = [10, 20, 30, 40, 50]  # Adjust as needed
        reg_param_values = [0.01]  # Adjust as needed

        for N_S in N_S_list:
            print("Subject:", N_S)

            # Load all trials for a single subject
            X, Y = extract_data_from_subject(root_dir, N_S, datatype)

            # Cut useful time. i.e action interval
            X = select_time_window(X=X, t_start=t_start, t_end=t_end, fs=fs)

            # Filter By condition i.e Inner
            X, Y = filter_by_condition(X, Y, condition='Inner')

            # Filter By class i.e ALL classes
            X, Y = filter_by_class(X, Y, 'ALL')

            for n_components in n_components_values:
                for reg_param in reg_param_values:
                    print("Trying n_components =", n_components, "and reg_param =", reg_param)

                    # Initialize CSP with regularization
                    csp = CSP(n_components=n_components, reg=reg_param, log=True, norm_trace=False)

                    try:
                        # Fit CSP on training data and transform data
                        X_csp = csp.fit_transform(X, Y)
                        print("SVD computation successful for n_components =", n_components, "and reg_param =", reg_param)

                        # Save the extracted features and labels
                        file_name = f"{save_dir}/CSP_Features_Subject_{N_S}_ncomp_{n_components}_reg_{reg_param}.npz"
                        np.savez(file_name, features=X_csp, labels=Y)
                        print(f"Saved CSP features to: {file_name}")

                    except Exception as e:
                        print("Error during SVD computation:", e)

    except Exception as e:
        print("Error:", e)

# Call the function to extract features
extract_features_using_csp(root_dir, N_S_list, t_start, t_end, fs)

"""# **--------------------------- Model Training \------------------------------------**

### Load and Inspect the numpy File
"""

import pandas as pd
import numpy as np

# file_name_csp='/content/drive/MyDrive/FinalFilterOuput/50_Combined_CSP_Features.npz'
file_name_csp="/content/drive/MyDrive/FinalFilterOuput/CombinedFile_6Subjects_50_testing_CSP_Features.npz"
# Load the .npz file
data = np.load(file_name_csp)

# List all arrays in the .npz file
print(data.files)

data['labels'].shape

labels= data['labels']
labels= labels[:, 1]
labels.shape

unique, frequency = np.unique(labels,
                              return_counts = True)

# convert both into one numpy array
count = np.asarray((unique, frequency ))

print("The values and their frequency are:\n",
     count)

"""
### Data Cleaning and preparation for numpy files"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
import joblib


# Assuming 'Feature' and 'Label' are keys in the npz file
features = data['features']

# Function to clean feature strings
def clean_feature_string(feature_str):
    if isinstance(feature_str, str):
        cleaned = feature_str.strip('[]').replace(' ', ',').replace('\n', '')
        return cleaned.split(',') if cleaned else []
    return feature_str  # If it's already a list or array, return as is

# Convert the feature strings to lists of floats
cleaned_features = []
for feature in features:
    cleaned_feature = clean_feature_string(feature)
    if len(cleaned_feature) == 0:
        cleaned_feature = [0.0] * 10  # Handle empty features by filling with placeholder
    cleaned_features.append([float(i) for i in cleaned_feature])

# Convert cleaned_features to a numpy array
X = np.array(cleaned_features)
y = labels

# Optional: Split the data into training and testing sets

'''
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert labels to categorical (if using TensorFlow/Keras)
num_classes = len(np.unique(y))
y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)

print("First few rows of scaled features:")
print(X_train[:5])
print("First few labels (one-hot encoded):")
print(y_train[:5])

print("Data cleaning and preparation complete.")
'''

num_classes = len(np.unique(y))
y = tf.keras.utils.to_categorical(y, num_classes=num_classes)
y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test= scaler.transform(X_test)

# Save the scaler to disk for later use
scaler_path = '/content/drive/MyDrive/MyOutput/Maham_final_final_training_scaler.pkl'  # Adjust the path as needed
joblib.dump(scaler, scaler_path)

np.unique(y)

"""### CNN MODEL"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization

# Reshape the features for CNN input
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Define the CNN model
model = Sequential([
    Conv1D(128, 3, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.7),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')  # Adjust the output layer based on the number of classes
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),
                    epochs=100, batch_size=16, callbacks=[early_stopping], verbose=2)

# Define the folder path and ensure it exists
folder_path = '/content/drive/MyDrive/MyOutput'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# Define the file path where the model will be saved
model_file_path = os.path.join(folder_path, 'maham_final_cnn_model.h5')

# Save the model
model.save(model_file_path)
print(f'Model saved to {model_file_path}')

# Evaluate the model
loss, accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)
print(f'CNN Model Accuracy: {accuracy:.2%}')

# Make predictions and evaluate
y_pred = model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

# Generate a classification report
from sklearn.metrics import classification_report
report = classification_report(y_test_classes, y_pred_classes)
print('CNN Model Classification Report:')
print(report)

"""# **------------------------- Test on Single file ----------------------**

## Code Divided Into Sub Section

### Import File
"""

import pandas as pd
import numpy as np

# csp_file_name='/content/drive/MyDrive/FinalFilterOuput/Subject8_50_testing_CSP_Features.npz' # 77.50%
# csp_file_name='/content/drive/MyDrive/FinalFilterOuput/Subject7_50_testing_CSP_Features.npz' # 77.50%
# csp_file_name='/content/drive/MyDrive/FinalFilterOuput/Subject9_50_testing_CSP_Features.npz' #97.08%
# csp_file_name='/content/drive/MyDrive/FinalFilterOuput/Subject2_50_testing_CSP_Features.npz' # 95.00%

csp_file_name='/content/drive/MyDrive/FinalFilterOuput/blocks/subject_2_block_3_testing_50_CSP_Features.npz' #35.42%
# Load the .npz file
new_data = np.load(csp_file_name)
# List all arrays in the .npz file
print(new_data.files)

# Access and print each array

for array_name in new_data.files:
    print(array_name)
    print(new_data[array_name])

new_data['features'].shape

"""### Displaying Features and Labels"""

labels= new_data['labels']
labels= labels[:, 1]
print("Labels Shape ----> ", labels.shape)

unique, frequency = np.unique(labels,
                              return_counts = True)

# convert both into one numpy array
count = np.asarray((unique, frequency ))

print("The values and their frequency are:\n",count)

"""### Preprocess and clean npz"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
import joblib



# Load the .npz file

# Assuming 'Feature' and 'Label' are keys in the npz file
features = new_data['features']

# Function to clean feature strings
def clean_feature_string(feature_str):
    if isinstance(feature_str, str):
        cleaned = feature_str.strip('[]').replace(' ', ',').replace('\n', '')
        return cleaned.split(',') if cleaned else []
    return feature_str  # If it's already a list or array, return as is

# Convert the feature strings to lists of floats
cleaned_features = []
for feature in features:
    cleaned_feature = clean_feature_string(feature)
    if len(cleaned_feature) == 0:
        cleaned_feature = [0.0] * 10  # Handle empty features by filling with placeholder
    cleaned_features.append([float(i) for i in cleaned_feature])

# Convert cleaned_features to a numpy array
X_new = np.array(cleaned_features)
y_new = labels

"""###  Standardize features

"""

# Standardize features
scaler_path = '/content/drive/MyDrive/MyOutput/Maham_final_final_training_scaler.pkl'
scaler = joblib.load(scaler_path)

X_new = scaler.transform(X_new)
print(X_new.shape)

# Reshape the data for the model if necessary (e.g., CNN input)
X_new_transformed = X_new.reshape(X_new.shape[0], X_new.shape[1], 1)
print(X_new_transformed.shape)

"""### Import Model"""

from tensorflow.keras.models import load_model


# Load the trained model
model_path ='/content/drive/MyDrive/MyOutput/maham_final_cnn_model.h5'


model = load_model(model_path)
print(f"Model loaded from {model_path}")

"""### Test Model"""

# Assuming model and data are already loaded and prepared
from sklearn.metrics import accuracy_score
predictions = model.predict(X_new_transformed)
predicted_classes = np.argmax(predictions, axis=1)
accuracy = accuracy_score(y_new, predicted_classes)
print("Overall accuracy on test data: {:.2%}".format(accuracy))

"""## Statistical Analysis

### Classification Report:
"""

from sklearn.metrics import classification_report
report = classification_report(y_new, predicted_classes)
print('CNN Model Classification Report:')
print(report)

"""### Confusion Matrix:"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_new, predicted_classes)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_new), yticklabels=np.unique(y_new))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""### Precision, Recall, F1-Score:"""

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate precision, recall, f1-score
precision = precision_score(y_new, predicted_classes, average='weighted')
recall = recall_score(y_new, predicted_classes, average='weighted')
f1 = f1_score(y_new, predicted_classes, average='weighted')

print(f"Precision: {precision:.2%}")
print(f"Recall: {recall:.2%}")
print(f"F1-Score: {f1:.2%}")

"""### ROC Curve and AUC:

"""

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the labels for multi-class ROC AUC
y_new_binarized = label_binarize(y_new, classes=np.unique(y_new))
n_classes = y_new_binarized.shape[1]

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_new_binarized[:, i], predictions[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
plt.figure(figsize=(10, 7))
colors = plt.cm.get_cmap('tab10', n_classes)
for i, color in zip(range(n_classes), colors(range(n_classes))):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend(loc='lower right')
plt.show()

"""### Class Distribution:"""

# Checking the distribution of the predicted classes
unique, counts = np.unique(predicted_classes, return_counts=True)
class_distribution = dict(zip(unique, counts))

print("Class Distribution in Predictions:")
for key, value in class_distribution.items():
    print(f"Class {key}: {value} samples")

"""### Statistical Analysis Report:"""

# Create a summary report
summary_report = {
    "accuracy": accuracy,
    "precision": precision,
    "recall": recall,
    "f1_score": f1,
    "class_distribution": class_distribution
}

# Convert to DataFrame for better visualization
summary_df = pd.DataFrame(summary_report, index=[0])
print("Statistical Analysis Summary Report:")
print(summary_df)

"""## Full Code to Test a Single File on Model"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
import joblib
from tensorflow.keras.models import load_model

csp_file_name='/content/drive/MyDrive/FinalFilterOuput/blocks/subject_2_block_3_testing_50_CSP_Features.npz' #35.42%

# Load the .npz file
new_data = np.load(csp_file_name)


new_data['features'].shape


# Preprocess and clean numpy file
labels= new_data['labels']
labels= labels[:, 1]
labels.shape


unique, frequency = np.unique(labels,
                              return_counts = True)

# convert both into one numpy array
count = np.asarray((unique, frequency ))

print("The values and their frequency are:\n",count)




# Load the .npz file

# Assuming 'Feature' and 'Label' are keys in the npz file
features = new_data['features']

# Function to clean feature strings
def clean_feature_string(feature_str):
    if isinstance(feature_str, str):
        cleaned = feature_str.strip('[]').replace(' ', ',').replace('\n', '')
        return cleaned.split(',') if cleaned else []
    return feature_str  # If it's already a list or array, return as is

# Convert the feature strings to lists of floats
cleaned_features = []
for feature in features:
    cleaned_feature = clean_feature_string(feature)
    if len(cleaned_feature) == 0:
        cleaned_feature = [0.0] * 10  # Handle empty features by filling with placeholder
    cleaned_features.append([float(i) for i in cleaned_feature])

# Convert cleaned_features to a numpy array
X_new = np.array(cleaned_features)
y_new = labels


# Standardize features
scaler_path = '/content/drive/MyDrive/MyOutput/final_training_scaler.pkl'
scaler = joblib.load(scaler_path)

X_new = scaler.transform(X_new)


print("Shape: ", X_new.shape)

# Reshape the data for the model if necessary (e.g., CNN input)
X_new_transformed = X_new.reshape(X_new.shape[0], X_new.shape[1], 1)

print("X_new_transformed.shape", X_new_transformed.shape)


# Load the trained model
model_path ='/content/drive/MyDrive/MyOutput/final_cnn_model.h5'


model = load_model(model_path)
print(f"Model loaded from {model_path}")


# Assuming model and data are already loaded and prepared
from sklearn.metrics import accuracy_score
predictions = model.predict(X_new_transformed)
predicted_classes = np.argmax(predictions, axis=1)
accuracy = accuracy_score(y_new, predicted_classes)
print("Overall accuracy on test data: {:.2%}".format(accuracy))

from sklearn.metrics import classification_report
report = classification_report(y_new, predicted_classes)
print('CNN Model Classification Report:')
print(report)

"""# **------------------------- Test multiple files ----------------------**"""

import os
import numpy as np
import joblib
import tensorflow as tf
from sklearn.metrics import accuracy_score, classification_report

# Paths
blocks_folder_path = '/content/drive/MyDrive/FinalFilterOuput/subjects/all_csp_50_subjects'
scaler_path = '/content/drive/MyDrive/MyOutput/all_subjects_training_scaler.pkl'
model_path = '/content/drive/MyDrive/MyOutput/all_subjects_training_file_cnn_model.h5'
output_txt_path = '/content/drive/MyDrive/FinalFilterOuput/subjects/all_csp_50_subjects/all_subjects_model_results.txt'

# Load the trained model
model = tf.keras.models.load_model(model_path)
print(f"Model loaded from {model_path}")

# Load the scaler
scaler = joblib.load(scaler_path)
print(f"Scaler loaded from {scaler_path}")

# Function to clean feature strings
def clean_feature_string(feature_str):
    if isinstance(feature_str, str):
        cleaned = feature_str.strip('[]').replace(' ', ',').replace('\n', '')
        return cleaned.split(',') if cleaned else []
    return feature_str  # If it's already a list or array, return as is

# Initialize a list to hold results
results = []

# Iterate through all files in the blocks folder
for root, dirs, files in os.walk(blocks_folder_path):
    for file in files:
        if file.endswith('.npz'):
            file_path = os.path.join(root, file)
            print(f"Processing file: {file_path}")

            # Load the .npz file
            new_data = np.load(file_path)

            # Extract features and labels
            features = new_data['features']
            labels = new_data['labels'][:, 1]  # Assuming the second column is the class label

            # Convert the feature strings to lists of floats
            cleaned_features = []
            for feature in features:
                cleaned_feature = clean_feature_string(feature)
                if len(cleaned_feature) == 0:
                    cleaned_feature = [0.0] * 10  # Handle empty features by filling with placeholder
                cleaned_features.append([float(i) for i in cleaned_feature])

            # Convert cleaned_features to a numpy array
            X_new = np.array(cleaned_features)
            y_new = labels

            # Standardize features
            X_new = scaler.transform(X_new)

            # Reshape the data for the model if necessary (e.g., CNN input)
            X_new_transformed = X_new.reshape(X_new.shape[0], X_new.shape[1], 1)

            # Make predictions and evaluate
            predictions = model.predict(X_new_transformed)
            predicted_classes = np.argmax(predictions, axis=1)
            accuracy = accuracy_score(y_new, predicted_classes)
            report = classification_report(y_new, predicted_classes)

            # Print the accuracy
            print(f"Accuracy for {file}: {accuracy:.2%}")

            # Save the result
            results.append(f"File: {file}\nAccuracy: {accuracy:.2%}\nClassification Report:\n{report}\n\n")

# Write all results to the output text file
with open(output_txt_path, 'w') as f:
    for result in results:
        f.write(result)

print(f"Results saved to {output_txt_path}")

"""# **------------------------- EEG FILE CLEANING ----------------------**"""

import mne
import pickle
import numpy as np

from Inner_Speech_Dataset.Python_Processing.Events_analysis import Event_correction, add_condition_tag, add_block_tag
from Inner_Speech_Dataset.Python_Processing.Events_analysis import check_baseline_tags, delete_trigger
from Inner_Speech_Dataset.Python_Processing.Events_analysis import cognitive_control_check, standardize_labels
# from Inner_Speech_Dataset.Python_Processing.Data_extractions import extract_subject_from_bdf
from Inner_Speech_Dataset.Python_Processing.Utilitys import ensure_dir,sub_name, unify_names
# from utils.Utilitys import sub_name, unify_names       # noqa




# Processing Variables

# Root where the raw data are stored
root_dir = '/content/drive/MyDrive/'

# Root where the structured data will be saved
# It can be changed and saved in other direction
save_dir = root_dir + "derivatives/"

# #################### Filtering
# Cut-off frequencies
Low_cut = 0.5
High_cut = 100

# Notch filter in 50Hz
Notch_bool = True

# Downsampling rate
DS_rate = 4

# #################### ICA
# If False, ICA is not applied
ICA_bool = True
ICA_Components = None
ica_random_state = 23
ica_method = 'infomax'
max_pca_components = None
fit_params = dict(extended=True)

# #################### EMG Control
low_f = 1
high_f = 20
# Slide window design
# Window len (time in sec)
window_len = 0.5
# slide window step (time in sec)
window_step = 0.05

# Threshold for EMG control
std_times = 3

# Baseline
t_min_baseline = 0
t_max_baseline = 15

# Trial time
t_min = 1
t_max = 3.5

# Events ID
# Trials tag for each class.
# 31 = Arriba / Up
# 32 = Abajo / Down
# 33 = Derecha / Right
# 34 = Izquierda / Left
event_id = dict(Arriba=31, Abajo=32, Derecha=33, Izquierda=34)

# Baseline id
baseline_id = dict(Baseline=13)

# Report initialization
report = dict(Age=0, Gender=0, Recording_time=0, Ans_R=0, Ans_W=0)

# Montage
Adquisition_eq = "biosemi128"
# Get montage
montage = mne.channels.make_standard_montage(Adquisition_eq)

# Extern channels
Ref_channels = ['EXG1', 'EXG2']

# Gaze detection
Gaze_channels = ['EXG3', 'EXG4']

# Blinks detection
Blinks_channels = ['EXG5', 'EXG6']

# Mouth Moving detection
Mouth_channels = ['EXG7', 'EXG8']

# Demographic information
Subject_age = [56, 50, 34, 24, 31, 29, 26, 28, 35, 31]
Subject_gender = ['F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M']

def extract_subject_from_bdf(local_file_path,N_S, N_B):
    # Name correction if N_Subj is less than 10
    num_s = sub_name(N_S)


    file_name=local_file_path
    print("this is the file name from data extraction",file_name)

    raw_data = mne.io.read_raw_bdf(input_fname=file_name, preload=True,
                                   verbose='WARNING')
    print("this is the raw data",raw_data)
    return raw_data, num_s

def process_subject_session(local_file_path, subject_no, session_no):

    N_S = subject_no
    N_B = session_no

    print(type(N_S))
    print(type(N_B))

    # Initialize report
    report = dict()

    # Get Age and Gender
    report['Age'] = Subject_age[N_S-1]
    report['Gender'] = Subject_gender[N_S-1]

    print('Subject: ' + str(N_S))
    print('Session: ' + str(N_B))

    # Load data from BDF file
    rawdata, Num_s = extract_subject_from_bdf(local_file_path, N_S, N_B)

    print('Data loaded')
    print('Referencing')

    # Referencing
    rawdata.set_eeg_reference(ref_channels=Ref_channels)

    print('Montage')
    if Notch_bool:
        # Notch filter
        rawdata = mne.io.Raw.notch_filter(rawdata, freqs=50)

    print(rawdata.info)

    # Filtering raw data
    print('Filtering')
    rawdata.filter(Low_cut, High_cut)

    # Get events
    # Subject 10 on Block 1 have a spurious trigger
    if (N_S == 10 and N_B == 1):
        events = mne.find_events(rawdata, initial_event=True,
                                 consecutive=True, min_duration=0.002)
    else:
        events = mne.find_events(rawdata, initial_event=True,
                                 consecutive=True)

    events = check_baseline_tags(events)

    # Check and Correct event
    events = Event_correction(events=events)

    # Replace the raw events with the new corrected events
    rawdata.event = events

    report['Recording_time'] = int(np.round(rawdata.last_samp/rawdata.info['sfreq']))

    # Cognitive Control
    report['Ans_R'], report['Ans_W'] = cognitive_control_check(events)

    # Save report
    file_path = save_dir + Num_s + '/ses-0' + str(N_B)
    ensure_dir(file_path)

    pickle_file_name = file_path + '/' + Num_s + '_ses-0'+str(N_B)+'_report.pkl'
    with open(pickle_file_name, 'wb') as output:
        pickle.dump(report, output, pickle.HIGHEST_PROTOCOL)

    # EXG
    picks_eog = mne.pick_types(rawdata.info, eeg=False, stim=False, include=['EXG1', 'EXG2', 'EXG3', 'EXG4','EXG5', 'EXG6', 'EXG7', 'EXG8'])

    epochsEOG = mne.Epochs(rawdata, events, event_id=event_id, tmin=-0.5,
                               tmax=4, picks=picks_eog, preload=True,
                               detrend=0, decim=DS_rate)

    # Save EOG
    eog_file_name = file_path + '/' + Num_s + '_ses-0' + str(N_B) + '_exg-epo.fif'
    epochsEOG.save(eog_file_name, fmt='double',
                       split_size='2GB', overwrite=True)

    # Baseline
    t_baseline = (events[events[:, 2] == 14, 0]-events[events[:, 2] == 13, 0]) / rawdata.info['sfreq']
    t_baseline = t_baseline[0]
    Baseline = mne.Epochs(rawdata, events, event_id=baseline_id, tmin=0,
                              tmax=round(t_baseline), picks='all',
                              preload=True, detrend=0, decim=DS_rate,
                              baseline=None)

    # Save Baseline
    baseline_file_name = file_path + '/' + Num_s + '_ses-0' + str(N_B) + '_baseline-epo.fif'
    Baseline.save(baseline_file_name, fmt='double',
                      split_size='2GB', overwrite=True)

    # Epoching and decimating EEG
    picks_eeg = mne.pick_types(rawdata.info, eeg=True,
                                   exclude=['EXG1', 'EXG2', 'EXG3', 'EXG4',
                                            'EXG5', 'EXG6', 'EXG7', 'EXG8'],
                                   stim=False)

    epochsEEG = mne.Epochs(rawdata, events, event_id=event_id, tmin=-0.5,
                               tmax=4, picks=picks_eeg, preload=True,
                               detrend=0, decim=DS_rate, baseline=None)

    # ICA Processing
    if ICA_bool:
        # Get a full trials including EXG channels
        picks_vir = mne.pick_types(rawdata.info, eeg=True,
                                   include=['EXG1', 'EXG2', 'EXG3', 'EXG4',
                                            'EXG5', 'EXG6',
                                            'EXG7', 'EXG8'],
                                   stim=False)
        epochsEEG_full = mne.Epochs(rawdata, events, event_id=event_id,
                                    tmin=-0.5, tmax=4,
                                    picks=picks_vir, preload=True,
                                    detrend=0, decim=DS_rate,
                                    baseline=None)

        # Liberate Memory for ICA processing
        del rawdata

        # Creating the ICA object
        ica = mne.preprocessing.ICA(n_components=ICA_Components,
                                    random_state=ica_random_state,
                                    method=ica_method,
                                    fit_params=fit_params)

        # Fit ICA, calculate components
        ica.fit(epochsEEG)
        ica.exclude = []

        # Detect sources by correlation
        exg_inds_EXG3, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG3')
        ica.exclude.extend(exg_inds_EXG3)

        # Detect sources by correlation
        exg_inds_EXG4, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG4')
        ica.exclude.extend(exg_inds_EXG4)

        # Detect sources by correlation
        exg_inds_EXG5, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG5')
        ica.exclude.extend(exg_inds_EXG5)

        # Detect sources by correlation
        exg_inds_EXG6, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG6')
        ica.exclude.extend(exg_inds_EXG6)

        # Detect sources by correlation
        exg_inds_EXG7, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG7')
        ica.exclude.extend(exg_inds_EXG7)

        # Detect sources by correlation
        exg_inds_EXG8, scores_ica = ica.find_bads_eog(epochsEEG_full,
                                                      ch_name='EXG8')
        ica.exclude.extend(exg_inds_EXG8)

        print("Applying ICA")
        ica.apply(epochsEEG)

    # Save EEG
    eeg_file_name = file_path + '/' + Num_s + '_ses-0' + str(N_B) + '_eeg-epo.fif'
    epochsEEG.save(eeg_file_name, fmt='double',
                   split_size='2GB', overwrite=True)

    # Standardize and save events
    events = add_condition_tag(events)
    events = add_block_tag(events, N_B=N_B)
    events = delete_trigger(events)
    events = standardize_labels(events)

    # Save events
    events_file_name = file_path + '/' + Num_s + '_ses-0' + str(N_B) + '_events.dat'
    events.dump(events_file_name)

    return {
        'eog_file': eog_file_name,
        'baseline_file': baseline_file_name,
        'pickle_file': pickle_file_name,
        'eeg_file': eeg_file_name,
        'events_file': events_file_name
    }

# Example call
local_file = '/content/drive/MyDrive/FYPD_Dataset/sub-02/ses-02/eeg/sub-02_ses-02_task-innerspeech_eeg.bdf'
file_paths =
print(file_paths)


results = process_subject_session(local_file, 2, 1)
print("this is the result",results)